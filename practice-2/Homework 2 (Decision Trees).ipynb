{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWwskeoOgUB5"
   },
   "source": [
    "# Завдання"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3mc9IO2gUB5"
   },
   "source": [
    "_Вправа 1: натренуйте модель дерева рішень та виконайте пошук гіперпараметрів для moons dataset._ \\\n",
    "a. Згенеруйте датасет, використовуючи функцію `make_moons(n_samples=10000, noise=0.4)`. \\\n",
    "b. Розбийте її на тренувальну та тестувальну частини, використовуючи функцію `train_test_split()`. \\\n",
    "c. Використайте пошук з крос-валідацією (`GridSearchCV`, `RandomizedSearchCV`) для пошуку гіперпараметрів моделі `DecisionTreeClassifier`. Зокрема, спробуйте різні значення для параметра `max_leaf_nodes`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b88Ac8nVgeVL"
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform\n",
    "from matplotlib.colors import ListedColormap\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_decision_boundaries(\n",
    "        clf, X, ax=None, resolution=300, cmap=plt.cm.coolwarm, alpha=0.3\n",
    "):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution),\n",
    "        np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=alpha, cmap=cmap)\n",
    "    return ax"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cUpCtqUDgyzD"
   },
   "source": [
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLwgp5R2EGIA"
   },
   "source": [
    "After searching for best hyperparameters for the model, the results show that the best max_leaf_nodes number is 10 for both approaches. The other parameters differ."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_0, X_1 = X[y == 0], X[y == 1]\n",
    "print(X_0.shape, X_1.shape)\n",
    "print(\n",
    "    X_train[y_train == 0].shape,\n",
    "    X_train[y_train == 1].shape\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_points(X_0, X_1, ax=None, alpha=0.1):\n",
    "    X_0, X_1 = X_0.T, X_1.T\n",
    "    getattr(ax or plt, \"scatter\")(X_0[0], X_0[1], 0.3, \"b\", alpha=alpha)\n",
    "    getattr(ax or plt, \"scatter\")(X_1[0], X_1[1], 0.3, \"y\", alpha=alpha)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_points(X_0, X_1)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dt_classifier = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=5\n",
    ")\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "y_train_pred = dt_classifier.predict(X_train)\n",
    "y_test_pred = dt_classifier.predict(X_test)\n",
    "print(accuracy_score(y_train, y_train_pred))\n",
    "print(accuracy_score(y_test, y_test_pred))\n",
    "plt.figure(figsize=(12, 6))\n",
    "tree.plot_tree(dt_classifier)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_0_train, X_1_train = X_train[y_train == 0], X_train[y_train == 1]\n",
    "X_0_test, X_1_test = X_test[y_test == 0], X_test[y_test == 1]\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plot_points(X_0_train, X_1_train, ax, alpha=0.2)\n",
    "plot_decision_boundaries(\n",
    "    dt_classifier, X_train, ax=ax, alpha=0.1,\n",
    "    cmap=plt.cm.Wistia\n",
    ")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "search_model = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    {\n",
    "        \"min_samples_split\": (1, 3, 5, 8),\n",
    "        \"min_samples_leaf\": (8, 12, 15, 20),\n",
    "        \"max_depth\": (2, 5, 8, 10, 12),\n",
    "        \"max_leaf_nodes\": (15, 18, 22, 26, 28)\n",
    "    },\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "search_model.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bp = search_model.best_params_\n",
    "print(\n",
    "    f\"Best score: {search_model.best_score_}\\n\"\n",
    "    f\"Best params:\\n\"\n",
    "    f\"\\tmax_depth:{bp['max_depth']}\\n\"\n",
    "    f\"\\tmin_samples_split:{bp['min_samples_split']}\\n\"\n",
    "    f\"\\tmin_samples_leaf:{bp['min_samples_leaf']}\\n\"\n",
    "    f\"\\tmax_leaf_nodes:{bp['max_leaf_nodes']}\"\n",
    ")\n",
    "print(f\"Test split accuracy: {accuracy_score(y_test, search_model.predict(X_test))}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dtc_best = DecisionTreeClassifier(random_state=42, **search_model.best_params_)\n",
    "dtc_best.fit(X_train, y_train)\n",
    "plt.figure(figsize=(12, 6))\n",
    "tree.plot_tree(dtc_best)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plot_points(X_0_train, X_1_train, ax, alpha=0.2)\n",
    "plot_decision_boundaries(\n",
    "    search_model, X_train, ax=ax, alpha=0.1,\n",
    "    cmap=plt.cm.Wistia\n",
    ")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJGqlxlZgUB6"
   },
   "source": [
    "_Вправа 2_ : \\\n",
    "a. Завантажте датасет MNIST та розбийте його на тренувальну, валідаційну та тестувальну частини (наприклад, 50 000 / 10 000 / 10\n",
    "000). \\\n",
    "b. Натренуйте різні класифікаційні моделі (Random Forest classifier, Logistic Regression, SVM). \\\n",
    "c. Далі, об'єднайте їх за допомогою голосування. \\\n",
    "d. Натренуйте нову модель, використовуючи виходи попередніх моделей на валідаційній вибірці (це будуть нові ознаки і нова тренувальна вибірка для даної моделі). Протестуйте отриманий ланцюжок на тренувальний вибірці. Такий спосіб поєднання моделей називається стогуванням (stacking).  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfcKwDZhlk3U",
    "outputId": "46f47c7e-b75d-41cf-820b-0f6c83f5cef1"
   },
   "source": [
    "# from sklearn.datasets import fetch_openml\n",
    "#\n",
    "# mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "#\n",
    "# X = mnist[\"data\"]\n",
    "# y = mnist[\"target\"].astype(np.uint8)\n",
    "#\n",
    "# X_train = X[:50000]\n",
    "# y_train = y[:50000]\n",
    "# X_test = X[60000:]\n",
    "# y_test = y[60000:]\n",
    "#\n",
    "# X_val = X[50000:60000]\n",
    "# y_val = y[50000:60000]"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jJYdT5_EgUBQ",
    "pOq0OsAegUBa",
    "Mnw4jQMQgUBc",
    "1u40dmfNgUBe",
    "kzUdxxYtgUBm",
    "xyvivmPwgUBq",
    "3dP_uFbrgUBv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
